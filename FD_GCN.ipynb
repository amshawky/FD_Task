{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d7b19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "import dgl\n",
    "import torch as th\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7aed7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction=pd.read_csv('dataset/train_transaction.csv')\n",
    "train_identity=pd.read_csv('dataset/train_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8268f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the two tables using TransactionID where the resulting dataframe contains the intersection of both frames.\n",
    "merged_df = pd.merge(train_transaction, train_identity, on='TransactionID', how='inner')\n",
    "#sort the merged dataframe and perfrom sorting inplace to use less memory\n",
    "merged_df.sort_values(by='TransactionDT', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8165a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into 80% train set and 20% test set\n",
    "tot_size = len(merged_df)\n",
    "train_size = int(tot_size*0.8)\n",
    "test_size  = tot_size - train_size\n",
    "train_df = merged_df.head(train_size)\n",
    "test_df  = merged_df.tail(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de1a5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially, define the graph as none and initiate a set of parameters for training. These parameters can be changed as desired\n",
    "train_g = None\n",
    "parameters = {\n",
    "            'n_layers': 2,  # number of graph layers\n",
    "            'n_epochs': 150,  # number of training epochs\n",
    "            'n_hidden': 16,  # number of hidden units\n",
    "            'dropout': 0.2,  # dropout rate\n",
    "            'weight_decay': 5e-05,  # L2 penalization term \n",
    "            'lr': 0.01,  # learning rate\n",
    "            'target_col': 'TransactionID',  # target (transaction-id) column\n",
    "            'node_cols': 'card1,card2,card3,card4,card5,card6,ProductCD,addr1,addr2,P_emaildomain,R_emaildomain',  # columns to create nodes\n",
    "            'label_col': 'isFraud',  # label column\n",
    "            # categorical feature columns\n",
    "            'cat_cols': 'M1,M2,M3,M4,M5,M6,M7,M8,M9,DeviceType,DeviceInfo,id_12,id_13,id_14,id_15,id_16,id_17,id_18,id_19,id_20,id_21,id_22,id_23,id_24,id_25,id_26,id_27,id_28,id_29,id_30,id_31,id_32,id_33,id_34,id_35,id_36,id_37,id_38',\n",
    "            # numerical feature columns\n",
    "            'num_cols': 'TransactionAmt,dist1,dist2,id_01,id_02,id_03,id_04,id_05,id_06,id_07,id_08,id_09,id_10,id_11,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,D1,D2,D3,D4,D5,D6,D7,D8,D9,D10,D11,D12,D13,D14,D15,V1,V2,V3,V4,V5,V6,V7,V8,V9,V10,V11,V12,V13,V14,V15,V16,V17,V18,V19,V20,V21,V22,V23,V24,V25,V26,V27,V28,V29,V30,V31,V32,V33,V34,V35,V36,V37,V38,V39,V40,V41,V42,V43,V44,V45,V46,V47,V48,V49,V50,V51,V52,V53,V54,V55,V56,V57,V58,V59,V60,V61,V62,V63,V64,V65,V66,V67,V68,V69,V70,V71,V72,V73,V74,V75,V76,V77,V78,V79,V80,V81,V82,V83,V84,V85,V86,V87,V88,V89,V90,V91,V92,V93,V94,V95,V96,V97,V98,V99,V100,V101,V102,V103,V104,V105,V106,V107,V108,V109,V110,V111,V112,V113,V114,V115,V116,V117,V118,V119,V120,V121,V122,V123,V124,V125,V126,V127,V128,V129,V130,V131,V132,V133,V134,V135,V136,V137,V138,V139,V140,V141,V142,V143,V144,V145,V146,V147,V148,V149,V150,V151,V152,V153,V154,V155,V156,V157,V158,V159,V160,V161,V162,V163,V164,V165,V166,V167,V168,V169,V170,V171,V172,V173,V174,V175,V176,V177,V178,V179,V180,V181,V182,V183,V184,V185,V186,V187,V188,V189,V190,V191,V192,V193,V194,V195,V196,V197,V198,V199,V200,V201,V202,V203,V204,V205,V206,V207,V208,V209,V210,V211,V212,V213,V214,V215,V216,V217,V218,V219,V220,V221,V222,V223,V224,V225,V226,V227,V228,V229,V230,V231,V232,V233,V234,V235,V236,V237,V238,V239,V240,V241,V242,V243,V244,V245,V246,V247,V248,V249,V250,V251,V252,V253,V254,V255,V256,V257,V258,V259,V260,V261,V262,V263,V264,V265,V266,V267,V268,V269,V270,V271,V272,V273,V274,V275,V276,V277,V278,V279,V280,V281,V282,V283,V284,V285,V286,V287,V288,V289,V290,V291,V292,V293,V294,V295,V296,V297,V298,V299,V300,V301,V302,V303,V304,V305,V306,V307,V308,V309,V310,V311,V312,V313,V314,V315,V316,V317,V318,V319,V320,V321,V322,V323,V324,V325,V326,V327,V328,V329,V330,V331,V332,V333,V334,V335,V336,V337,V338,V339',\n",
    "            'class_weight': 1.  # class weight. Can take any value other than 1 to give importance to one class over the other\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27fcd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to map numerical features to log scale to decrease their range and the huge differences between them (feature smoothing)\n",
    "def scale_numerical_features(data):\n",
    "    return np.log10(data + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49fbc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a lookup dictionary mapping nodes of a certain type to integer ids\n",
    "def nodes_to_ids(vals, lookup, new_node_id=0):\n",
    "    ret=[]\n",
    "    new_nodes=[]\n",
    "    new_vals=[]\n",
    "    for v in vals:\n",
    "        if v not in lookup:\n",
    "            lookup[v] = new_node_id\n",
    "            new_nodes.append(new_node_id)\n",
    "            new_vals.append(v)\n",
    "            new_node_id += 1\n",
    "\n",
    "        ret.append(lookup[v])\n",
    "\n",
    "    return ret, lookup, new_nodes, new_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee3b19a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train_data(data):\n",
    "    mean = th.mean(data, dim=0)\n",
    "    std = th.sqrt(th.sum((data - mean)**2, dim=0)/data.shape[0])\n",
    "    std = std.numpy()\n",
    "    std[np.isclose(std, 0.0)]=1.\n",
    "    std = th.from_numpy(std)\n",
    "    return mean, std, (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06a2d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_test_data(data, mean, std):\n",
    "    return (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f848e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to construct the graph using DGL library (dgl.heterograph)\n",
    "def construct_graph(train_df, target_col, node_cols, cat_cols, num_cols):\n",
    "\n",
    "#construct a transformer to pre-process the different columns of train dataset. Numerical cols are passed to (scale_numerical_features) function and categorical cols are one-hot-encoded\n",
    "        feature_processor= make_column_transformer(\n",
    "            (\n",
    "                #scale all numerical features to log scale to smooth the feature set\n",
    "                FunctionTransformer(scale_numerical_features),\n",
    "                num_cols.split(',')\n",
    "            ),\n",
    "            (\n",
    "                #One-hot-encode the categorical features\n",
    "                OneHotEncoder(handle_unknown='ignore', sparse=False),\n",
    "                cat_cols.split(',')\n",
    "            ),\n",
    "            remainder='drop'\n",
    "        )\n",
    "\n",
    "        feature_processor.fit(train_df)\n",
    "\n",
    "        # fill NaN values with 0\n",
    "        features = np.nan_to_num(feature_processor.transform(train_df), nan=0.)\n",
    "\n",
    "        # create edge lists to store the different edges of the graph\n",
    "        edgelists = {}\n",
    "\n",
    "        nodes_lookup = {}\n",
    "        nodes_lookup['target'] = {}\n",
    "\n",
    "        # map target column (TransactionID) to integer ids\n",
    "        target_nodes, target_lookup, target_new_nodes, _ = nodes_to_ids(train_df[target_col], nodes_lookup['target'], 0)\n",
    "        nodes_lookup['target'] = target_lookup\n",
    "\n",
    "        # create self-relation edges\n",
    "        edgelists[('target', 'self_relation', 'target')] = [(t, t) for t in target_nodes]\n",
    "\n",
    "        # map nodes of type nc to integer ids\n",
    "        for nc in node_cols.split(','):\n",
    "            nodes_lookup[nc]={}\n",
    "            nodes, lookup, new_nodes, _ = nodes_to_ids(train_df[nc], nodes_lookup[nc], 0)\n",
    "            nodes_lookup[nc] = lookup\n",
    "\n",
    "            # construct bidirectional edges between target nodes and nodes of type nc\n",
    "            elist = []\n",
    "            rlist = []\n",
    "            for s, t in zip(target_nodes, nodes):\n",
    "                elist.append((s, t))\n",
    "                rlist.append((t, s))\n",
    "\n",
    "            edgelists[('target', f'target<>{nc}', nc)] = elist\n",
    "            edgelists[(nc, f'{nc}<>target', 'target')] = rlist\n",
    "\n",
    "        # create the heterograph object from edge lists using DGL library\n",
    "        g = dgl.heterograph(edgelists)\n",
    "        print(\n",
    "            \"The heterograph was created successfully: \\n Node types {} \\n Edge types{}\".format(\n",
    "                g.ntypes, g.canonical_etypes))\n",
    "\n",
    "        #Normalize the training features and add them to the graph target nodes\n",
    "        g.nodes['target'].data['features'] = th.from_numpy(features.astype('float32'))\n",
    "\n",
    "        train_mean, train_stdev, features = normalize_train_data(th.from_numpy(features.astype('float32')))\n",
    "\n",
    "        g.nodes['target'].data['features'] = features\n",
    "\n",
    "        return g, nodes_lookup, feature_processor, train_mean, train_stdev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dc26ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amsha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329: RuntimeWarning: invalid value encountered in log10\n",
      "  result = func(self.values, **kwargs)\n",
      "C:\\Users\\amsha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\amsha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329: RuntimeWarning: invalid value encountered in log10\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heterograph was created successfully: \n",
      " Node types ['P_emaildomain', 'ProductCD', 'R_emaildomain', 'addr1', 'addr2', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'target'] \n",
      " Edge types[('P_emaildomain', 'P_emaildomain<>target', 'target'), ('ProductCD', 'ProductCD<>target', 'target'), ('R_emaildomain', 'R_emaildomain<>target', 'target'), ('addr1', 'addr1<>target', 'target'), ('addr2', 'addr2<>target', 'target'), ('card1', 'card1<>target', 'target'), ('card2', 'card2<>target', 'target'), ('card3', 'card3<>target', 'target'), ('card4', 'card4<>target', 'target'), ('card5', 'card5<>target', 'target'), ('card6', 'card6<>target', 'target'), ('target', 'self_relation', 'target'), ('target', 'target<>P_emaildomain', 'P_emaildomain'), ('target', 'target<>ProductCD', 'ProductCD'), ('target', 'target<>R_emaildomain', 'R_emaildomain'), ('target', 'target<>addr1', 'addr1'), ('target', 'target<>addr2', 'addr2'), ('target', 'target<>card1', 'card1'), ('target', 'target<>card2', 'card2'), ('target', 'target<>card3', 'card3'), ('target', 'target<>card4', 'card4'), ('target', 'target<>card5', 'card5'), ('target', 'target<>card6', 'card6')]\n"
     ]
    }
   ],
   "source": [
    "train_g, nodes_lookup, feature_processor, train_mean, train_stdev = construct_graph(train_df, parameters['target_col'], parameters['node_cols'], parameters['cat_cols'], parameters['num_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88f21618",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroRGCNLayer(nn.Module):\n",
    "    #in_size and out_size are the size of each input and output sample\n",
    "    def __init__(self, in_size, out_size, etypes):\n",
    "        #calls init method of superclass to perfrom both functions of super and subclass\n",
    "        super(HeteroRGCNLayer, self).__init__()\n",
    "        # A dictionary of linear modules to compute the layer weights specific to each realtion (edge type)\n",
    "        self.weight = nn.ModuleDict({\n",
    "                name: nn.Linear(in_size, out_size) for name in etypes\n",
    "            })\n",
    "    def forward(self, G, feat_dict):\n",
    "        # feat_dict is a dictionary of node features for each node type\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            # Compute W * h, where (W) is the weight matrix and (h) are the layer features\n",
    "            if srctype in feat_dict:\n",
    "                #wh is the result of passing the features of the source node to the linear module representing the relation (edge)\n",
    "                Wh = self.weight[etype](feat_dict[srctype])\n",
    "                # Save it in graph for message passing\n",
    "                G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
    "                # Specify per-relation message passing functions: (message_func, reduce_func). Mean reduction is done per relation (etype) to get a weighted average of the contribution of all node features for this specific relation\n",
    "                funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
    "        # After Message passing is done and per-relation reduction (mean) is perfromed, then results are summed over all etypes to account for the contribution of all relations. \n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        # return the updated node feature dictionary\n",
    "        return {ntype: G.nodes[ntype].data['h'] for ntype in G.ntypes if 'h' in G.nodes[ntype].data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6ce8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class defining the RGCN model using multiple HeteroRGCNLayers\n",
    "class HeteroRGCN(nn.Module):\n",
    "    def __init__(self, ntype_dict, etypes, in_size, hidden_size, out_size, n_layers, embedding_size):\n",
    "        super(HeteroRGCN, self).__init__()\n",
    "        # Use trainable node embeddings as featureless inputs.\n",
    "        embed_dict = {ntype: nn.Parameter(th.Tensor(num_nodes, in_size))\n",
    "                      for ntype, num_nodes in ntype_dict.items() if ntype != 'target'}\n",
    "        for key, embed in embed_dict.items():\n",
    "            nn.init.xavier_uniform_(embed)\n",
    "        self.embed = nn.ParameterDict(embed_dict)\n",
    "        # stack layers to create the model as desired\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(HeteroRGCNLayer(embedding_size, hidden_size, etypes))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(HeteroRGCNLayer(hidden_size, hidden_size, etypes))\n",
    "\n",
    "        # output layer\n",
    "        self.layers.append(nn.Linear(hidden_size, out_size))\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        # Use the created trainable embeddings for all node types. For target node type, use passed in user features\n",
    "        h_dict = {ntype: emb for ntype, emb in self.embed.items()}\n",
    "        h_dict['target'] = features\n",
    "\n",
    "        # pass through all layers\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            if i != 0:\n",
    "                h_dict = {k: F.leaky_relu(h) for k, h in h_dict.items()}\n",
    "            h_dict = layer(g, h_dict)\n",
    "\n",
    "        # get final target predcitions\n",
    "        return self.layers[-1](h_dict['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79953b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, optimizer, loss, features, labels, train_g, device, n_epochs, test_mask):\n",
    "    #Function to train the model layers and optimize its parameters through backward propagation\n",
    "\n",
    "    train_mask = th.logical_not(test_mask)\n",
    "    train_idx =  th.nonzero(train_mask, as_tuple=True)[0]\n",
    "\n",
    "    duration = []\n",
    "    for epoch in range(n_epochs):\n",
    "        tic = time.time()\n",
    "        loss_val = 0.\n",
    "\n",
    "        #get model initial predictions\n",
    "        pred = model(train_g, features.to(device))\n",
    "        #compute the loss\n",
    "        l = loss(th.index_select(pred, 0, train_idx), \n",
    "                 th.index_select(labels, 0, train_idx))\n",
    "\n",
    "        #perfrom backward propagation to upfate all the weights (parameters) using thier gradient w.r.t the loss function\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val += l\n",
    "\n",
    "        duration.append(time.time() - tic)\n",
    "        \n",
    "        preds = model(train_g, features.to(device))\n",
    "        preds = th.argmax(preds, dim=1).numpy()\n",
    "        train_mask = np.logical_not(test_mask.numpy().astype('bool'))\n",
    "        train_labels = labels.numpy()\n",
    "        train_labels = np.compress(train_mask, train_labels)\n",
    "        train_preds= np.compress(train_mask, preds)\n",
    "        \n",
    "        #get the model metrics. Only the f1 score is displayed (could be changed as desired)\n",
    "        cf_m = confusion_matrix(train_labels, train_preds)\n",
    "        precision = cf_m[1,1] / (cf_m[1,1] + cf_m[0,1] + 10e-5)\n",
    "        recall = cf_m[1,1] / (cf_m[1,1] + cf_m[1,0])\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 10e-5)\n",
    "        \n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | f1 {:.4f} \".format(epoch, np.mean(duration), loss_val, f1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7cea1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_graph(train_df: pd.DataFrame, params: Dict[str, Any] = None, test_mask: List[bool] = None):\n",
    "        #Function to configure the model used for training and initialize its parameters\n",
    "        \n",
    "        device = th.device('cpu')\n",
    "\n",
    "        in_feats = train_g.nodes['target'].data['features'].shape[1]\n",
    "        n_classes = 2\n",
    "\n",
    "        ntype_dict = {n_type: train_g.number_of_nodes(n_type) for n_type in train_g.ntypes}\n",
    "\n",
    "        model = HeteroRGCN(ntype_dict, train_g.etypes, in_feats, params['n_hidden'], n_classes, params['n_layers'], in_feats)\n",
    "        model = model.to(device)\n",
    "\n",
    "        print(\"Configured Model\")\n",
    "\n",
    "        class_weights = [1. / params['class_weight'],\n",
    "                         params['class_weight']]\n",
    "\n",
    "        train_labels = train_df[params['label_col']].values\n",
    "\n",
    "        if test_mask is None:\n",
    "            test_mask = np.zeros_like(train_labels, dtype='bool')\n",
    "        else:\n",
    "            test_mask = np.asarray(test_mask)\n",
    "\n",
    "        train_features = train_g.nodes['target'].data['features'].to(device)\n",
    "        train_labels = th.from_numpy(train_labels).long().to(device)\n",
    "        test_mask = th.from_numpy(test_mask).to(device)\n",
    "\n",
    "        loss = th.nn.CrossEntropyLoss(weight=th.tensor(class_weights).float())\n",
    "        optimizer = th.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "\n",
    "        print(\"Training Started\")\n",
    "        model = trainer(model, optimizer, loss, train_features, train_labels, train_g, device, params['n_epochs'], test_mask)\n",
    "\n",
    "        print(\"Training Finished\")\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5a297c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Model\n",
      "Training Started\n",
      "Epoch 00000 | Time(s) 6.5014 | Loss 0.4219 | f1 0.2273 \n",
      "Epoch 00001 | Time(s) 6.2087 | Loss 0.3224 | f1 0.2223 \n",
      "Epoch 00002 | Time(s) 6.0875 | Loss 1.2985 | f1 0.2677 \n",
      "Epoch 00003 | Time(s) 5.9608 | Loss 0.2699 | f1 0.0467 \n",
      "Epoch 00004 | Time(s) 5.9134 | Loss 0.5356 | f1 0.0982 \n",
      "Epoch 00005 | Time(s) 5.8552 | Loss 0.5974 | f1 0.2690 \n",
      "Epoch 00006 | Time(s) 5.8356 | Loss 0.6131 | f1 0.1830 \n",
      "Epoch 00007 | Time(s) 5.9256 | Loss 0.5301 | f1 0.2736 \n",
      "Epoch 00008 | Time(s) 5.9842 | Loss 0.4593 | f1 0.4053 \n",
      "Epoch 00009 | Time(s) 5.9646 | Loss 0.3579 | f1 0.5025 \n",
      "Epoch 00010 | Time(s) 5.9556 | Loss 0.2724 | f1 0.5691 \n",
      "Epoch 00011 | Time(s) 5.9455 | Loss 0.2641 | f1 0.5656 \n",
      "Epoch 00012 | Time(s) 5.9385 | Loss 0.2664 | f1 0.5407 \n",
      "Epoch 00013 | Time(s) 5.9519 | Loss 0.2546 | f1 0.5360 \n",
      "Epoch 00014 | Time(s) 5.9813 | Loss 0.2495 | f1 0.5479 \n",
      "Epoch 00015 | Time(s) 5.9687 | Loss 0.2239 | f1 0.5731 \n",
      "Epoch 00016 | Time(s) 5.9620 | Loss 0.1867 | f1 0.6094 \n",
      "Epoch 00017 | Time(s) 5.9644 | Loss 0.1544 | f1 0.6499 \n",
      "Epoch 00018 | Time(s) 6.0024 | Loss 0.1459 | f1 0.6453 \n",
      "Epoch 00019 | Time(s) 5.9969 | Loss 0.1421 | f1 0.6061 \n",
      "Epoch 00020 | Time(s) 6.0028 | Loss 0.1350 | f1 0.5484 \n",
      "Epoch 00021 | Time(s) 6.0031 | Loss 0.1384 | f1 0.5156 \n",
      "Epoch 00022 | Time(s) 6.0240 | Loss 0.1404 | f1 0.5358 \n",
      "Epoch 00023 | Time(s) 6.0260 | Loss 0.1368 | f1 0.5921 \n",
      "Epoch 00024 | Time(s) 6.0307 | Loss 0.1318 | f1 0.6483 \n",
      "Epoch 00025 | Time(s) 6.0229 | Loss 0.1313 | f1 0.6694 \n",
      "Epoch 00026 | Time(s) 6.0168 | Loss 0.1300 | f1 0.6673 \n",
      "Epoch 00027 | Time(s) 6.0068 | Loss 0.1221 | f1 0.6476 \n",
      "Epoch 00028 | Time(s) 5.9982 | Loss 0.1192 | f1 0.6446 \n",
      "Epoch 00029 | Time(s) 6.0258 | Loss 0.1193 | f1 0.6583 \n",
      "Epoch 00030 | Time(s) 6.0647 | Loss 0.1173 | f1 0.6837 \n",
      "Epoch 00031 | Time(s) 6.1068 | Loss 0.1149 | f1 0.6967 \n",
      "Epoch 00032 | Time(s) 6.1389 | Loss 0.1151 | f1 0.7036 \n",
      "Epoch 00033 | Time(s) 6.1298 | Loss 0.1147 | f1 0.6999 \n",
      "Epoch 00034 | Time(s) 6.1289 | Loss 0.1118 | f1 0.6915 \n",
      "Epoch 00035 | Time(s) 6.1224 | Loss 0.1104 | f1 0.6883 \n",
      "Epoch 00036 | Time(s) 6.1243 | Loss 0.1098 | f1 0.6995 \n",
      "Epoch 00037 | Time(s) 6.1190 | Loss 0.1080 | f1 0.7200 \n",
      "Epoch 00038 | Time(s) 6.1130 | Loss 0.1059 | f1 0.7328 \n",
      "Epoch 00039 | Time(s) 6.1103 | Loss 0.1054 | f1 0.7377 \n",
      "Epoch 00040 | Time(s) 6.1080 | Loss 0.1047 | f1 0.7360 \n",
      "Epoch 00041 | Time(s) 6.1101 | Loss 0.1025 | f1 0.7287 \n",
      "Epoch 00042 | Time(s) 6.1641 | Loss 0.1013 | f1 0.7270 \n",
      "Epoch 00043 | Time(s) 6.2311 | Loss 0.1006 | f1 0.7323 \n",
      "Epoch 00044 | Time(s) 6.2429 | Loss 0.0991 | f1 0.7457 \n",
      "Epoch 00045 | Time(s) 6.2476 | Loss 0.0976 | f1 0.7546 \n",
      "Epoch 00046 | Time(s) 6.2524 | Loss 0.0967 | f1 0.7598 \n",
      "Epoch 00047 | Time(s) 6.2493 | Loss 0.0952 | f1 0.7586 \n",
      "Epoch 00048 | Time(s) 6.2490 | Loss 0.0936 | f1 0.7594 \n",
      "Epoch 00049 | Time(s) 6.2479 | Loss 0.0926 | f1 0.7647 \n",
      "Epoch 00050 | Time(s) 6.2457 | Loss 0.0913 | f1 0.7759 \n",
      "Epoch 00051 | Time(s) 6.2450 | Loss 0.0899 | f1 0.7815 \n",
      "Epoch 00052 | Time(s) 6.2445 | Loss 0.0889 | f1 0.7847 \n",
      "Epoch 00053 | Time(s) 6.2392 | Loss 0.0874 | f1 0.7847 \n",
      "Epoch 00054 | Time(s) 6.2368 | Loss 0.0861 | f1 0.7849 \n",
      "Epoch 00055 | Time(s) 6.2336 | Loss 0.0850 | f1 0.7924 \n",
      "Epoch 00056 | Time(s) 6.2300 | Loss 0.0836 | f1 0.8007 \n",
      "Epoch 00057 | Time(s) 6.2283 | Loss 0.0824 | f1 0.8047 \n",
      "Epoch 00058 | Time(s) 6.2280 | Loss 0.0811 | f1 0.8043 \n",
      "Epoch 00059 | Time(s) 6.2329 | Loss 0.0797 | f1 0.8064 \n",
      "Epoch 00060 | Time(s) 6.2315 | Loss 0.0786 | f1 0.8146 \n",
      "Epoch 00061 | Time(s) 6.2274 | Loss 0.0771 | f1 0.8192 \n",
      "Epoch 00062 | Time(s) 6.2276 | Loss 0.0759 | f1 0.8195 \n",
      "Epoch 00063 | Time(s) 6.2287 | Loss 0.0744 | f1 0.8226 \n",
      "Epoch 00064 | Time(s) 6.2249 | Loss 0.0731 | f1 0.8324 \n",
      "Epoch 00065 | Time(s) 6.2282 | Loss 0.0716 | f1 0.8392 \n",
      "Epoch 00066 | Time(s) 6.2256 | Loss 0.0700 | f1 0.8408 \n",
      "Epoch 00067 | Time(s) 6.2411 | Loss 0.0683 | f1 0.8471 \n",
      "Epoch 00068 | Time(s) 6.2454 | Loss 0.0664 | f1 0.8575 \n",
      "Epoch 00069 | Time(s) 6.2443 | Loss 0.0644 | f1 0.8635 \n",
      "Epoch 00070 | Time(s) 6.2462 | Loss 0.0619 | f1 0.8681 \n",
      "Epoch 00071 | Time(s) 6.2443 | Loss 0.0593 | f1 0.8818 \n",
      "Epoch 00072 | Time(s) 6.2567 | Loss 0.0566 | f1 0.8839 \n",
      "Epoch 00073 | Time(s) 6.2544 | Loss 0.0536 | f1 0.9039 \n",
      "Epoch 00074 | Time(s) 6.2528 | Loss 0.0509 | f1 0.8916 \n",
      "Epoch 00075 | Time(s) 6.2555 | Loss 0.0504 | f1 0.9147 \n",
      "Epoch 00076 | Time(s) 6.2750 | Loss 0.0494 | f1 0.8731 \n",
      "Epoch 00077 | Time(s) 6.2702 | Loss 0.0553 | f1 0.9198 \n",
      "Epoch 00078 | Time(s) 6.2645 | Loss 0.0439 | f1 0.8725 \n",
      "Epoch 00079 | Time(s) 6.2609 | Loss 0.0612 | f1 0.8506 \n",
      "Epoch 00080 | Time(s) 6.2568 | Loss 0.0681 | f1 0.8550 \n",
      "Epoch 00081 | Time(s) 6.2582 | Loss 0.0788 | f1 0.9077 \n",
      "Epoch 00082 | Time(s) 6.2604 | Loss 0.0475 | f1 0.8741 \n",
      "Epoch 00083 | Time(s) 6.2593 | Loss 0.0559 | f1 0.9089 \n",
      "Epoch 00084 | Time(s) 6.2569 | Loss 0.0487 | f1 0.9209 \n",
      "Epoch 00085 | Time(s) 6.2541 | Loss 0.0423 | f1 0.9000 \n",
      "Epoch 00086 | Time(s) 6.2511 | Loss 0.0505 | f1 0.9076 \n",
      "Epoch 00087 | Time(s) 6.2497 | Loss 0.0470 | f1 0.9282 \n",
      "Epoch 00088 | Time(s) 6.2461 | Loss 0.0388 | f1 0.9114 \n",
      "Epoch 00089 | Time(s) 6.2432 | Loss 0.0454 | f1 0.9331 \n",
      "Epoch 00090 | Time(s) 6.2391 | Loss 0.0368 | f1 0.9351 \n",
      "Epoch 00091 | Time(s) 6.2348 | Loss 0.0352 | f1 0.9253 \n",
      "Epoch 00092 | Time(s) 6.2305 | Loss 0.0376 | f1 0.9294 \n",
      "Epoch 00093 | Time(s) 6.2267 | Loss 0.0354 | f1 0.9425 \n",
      "Epoch 00094 | Time(s) 6.2228 | Loss 0.0328 | f1 0.9438 \n",
      "Epoch 00095 | Time(s) 6.2195 | Loss 0.0343 | f1 0.9488 \n",
      "Epoch 00096 | Time(s) 6.2163 | Loss 0.0301 | f1 0.9446 \n",
      "Epoch 00097 | Time(s) 6.2133 | Loss 0.0297 | f1 0.9421 \n",
      "Epoch 00098 | Time(s) 6.2114 | Loss 0.0302 | f1 0.9491 \n",
      "Epoch 00099 | Time(s) 6.2082 | Loss 0.0283 | f1 0.9534 \n",
      "Epoch 00100 | Time(s) 6.2053 | Loss 0.0284 | f1 0.9558 \n",
      "Epoch 00101 | Time(s) 6.2022 | Loss 0.0270 | f1 0.9557 \n",
      "Epoch 00102 | Time(s) 6.2041 | Loss 0.0254 | f1 0.9520 \n",
      "Epoch 00103 | Time(s) 6.2083 | Loss 0.0259 | f1 0.9561 \n",
      "Epoch 00104 | Time(s) 6.2081 | Loss 0.0250 | f1 0.9605 \n",
      "Epoch 00105 | Time(s) 6.2120 | Loss 0.0243 | f1 0.9618 \n",
      "Epoch 00106 | Time(s) 6.2118 | Loss 0.0242 | f1 0.9612 \n",
      "Epoch 00107 | Time(s) 6.2162 | Loss 0.0229 | f1 0.9585 \n",
      "Epoch 00108 | Time(s) 6.2270 | Loss 0.0231 | f1 0.9605 \n",
      "Epoch 00109 | Time(s) 6.2304 | Loss 0.0225 | f1 0.9643 \n",
      "Epoch 00110 | Time(s) 6.2390 | Loss 0.0220 | f1 0.9663 \n",
      "Epoch 00111 | Time(s) 6.2409 | Loss 0.0216 | f1 0.9654 \n",
      "Epoch 00112 | Time(s) 6.2403 | Loss 0.0207 | f1 0.9640 \n",
      "Epoch 00113 | Time(s) 6.2512 | Loss 0.0207 | f1 0.9672 \n",
      "Epoch 00114 | Time(s) 6.2503 | Loss 0.0201 | f1 0.9696 \n",
      "Epoch 00115 | Time(s) 6.2506 | Loss 0.0198 | f1 0.9702 \n",
      "Epoch 00116 | Time(s) 6.2506 | Loss 0.0193 | f1 0.9678 \n",
      "Epoch 00117 | Time(s) 6.2480 | Loss 0.0191 | f1 0.9682 \n",
      "Epoch 00118 | Time(s) 6.2456 | Loss 0.0189 | f1 0.9704 \n",
      "Epoch 00119 | Time(s) 6.2507 | Loss 0.0189 | f1 0.9721 \n",
      "Epoch 00120 | Time(s) 6.2560 | Loss 0.0191 | f1 0.9628 \n",
      "Epoch 00121 | Time(s) 6.2549 | Loss 0.0212 | f1 0.9683 \n",
      "Epoch 00122 | Time(s) 6.2544 | Loss 0.0203 | f1 0.9699 \n",
      "Epoch 00123 | Time(s) 6.2568 | Loss 0.0168 | f1 0.9655 \n",
      "Epoch 00124 | Time(s) 6.2561 | Loss 0.0195 | f1 0.9663 \n",
      "Epoch 00125 | Time(s) 6.2611 | Loss 0.0188 | f1 0.9662 \n",
      "Epoch 00126 | Time(s) 6.2613 | Loss 0.0183 | f1 0.9720 \n",
      "Epoch 00127 | Time(s) 6.2613 | Loss 0.0170 | f1 0.9730 \n",
      "Epoch 00128 | Time(s) 6.2657 | Loss 0.0166 | f1 0.9724 \n",
      "Epoch 00129 | Time(s) 6.2653 | Loss 0.0163 | f1 0.9738 \n",
      "Epoch 00130 | Time(s) 6.2665 | Loss 0.0157 | f1 0.9750 \n",
      "Epoch 00131 | Time(s) 6.2676 | Loss 0.0155 | f1 0.9765 \n",
      "Epoch 00132 | Time(s) 6.2659 | Loss 0.0156 | f1 0.9758 \n",
      "Epoch 00133 | Time(s) 6.2659 | Loss 0.0156 | f1 0.9748 \n",
      "Epoch 00134 | Time(s) 6.2670 | Loss 0.0155 | f1 0.9777 \n",
      "Epoch 00135 | Time(s) 6.2655 | Loss 0.0150 | f1 0.9768 \n",
      "Epoch 00136 | Time(s) 6.2708 | Loss 0.0144 | f1 0.9775 \n",
      "Epoch 00137 | Time(s) 6.2768 | Loss 0.0139 | f1 0.9789 \n",
      "Epoch 00138 | Time(s) 6.2740 | Loss 0.0137 | f1 0.9793 \n",
      "Epoch 00139 | Time(s) 6.2736 | Loss 0.0138 | f1 0.9782 \n",
      "Epoch 00140 | Time(s) 6.2730 | Loss 0.0135 | f1 0.9798 \n",
      "Epoch 00141 | Time(s) 6.2750 | Loss 0.0133 | f1 0.9809 \n",
      "Epoch 00142 | Time(s) 6.2737 | Loss 0.0130 | f1 0.9806 \n",
      "Epoch 00143 | Time(s) 6.2716 | Loss 0.0128 | f1 0.9809 \n",
      "Epoch 00144 | Time(s) 6.2699 | Loss 0.0127 | f1 0.9824 \n",
      "Epoch 00145 | Time(s) 6.2716 | Loss 0.0125 | f1 0.9814 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00146 | Time(s) 6.2716 | Loss 0.0123 | f1 0.9818 \n",
      "Epoch 00147 | Time(s) 6.2722 | Loss 0.0121 | f1 0.9832 \n",
      "Epoch 00148 | Time(s) 6.2715 | Loss 0.0119 | f1 0.9831 \n",
      "Epoch 00149 | Time(s) 6.2705 | Loss 0.0117 | f1 0.9829 \n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_graph(train_df, params=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc477ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_graph(test_df, train_g, target_col, node_cols, nodes_lookup, feature_processor, train_mean, train_stdev):\n",
    "    #Function to extend the training graph using new test nodes and extract the relevant subgraph for inference based on the number of hops desired\n",
    "        \n",
    "        #Pre-process the features of test nodes\n",
    "        features = np.nan_to_num(feature_processor.transform(test_df), nan=0.)\n",
    "\n",
    "        added_nodes = {}\n",
    "\n",
    "        target_nodes, target_lookup, target_new_nodes, target_new_vals = nodes_to_ids(test_df[target_col],\n",
    "                                                                                         nodes_lookup['target'],\n",
    "                                                                                         train_g.number_of_nodes('target'))\n",
    "\n",
    "        target_new_nodes = set(target_new_nodes)\n",
    "\n",
    "        target_nodes_to_add= [t for t in target_nodes if t in target_new_nodes]\n",
    "        feature_sel= [True if t in target_new_nodes else False for t in target_nodes]\n",
    "\n",
    "        new_features = np.compress(feature_sel, features, axis=0)\n",
    "        new_features = normalize_test_data(th.from_numpy(new_features), train_mean, train_stdev)\n",
    "\n",
    "        if len(target_new_nodes)> 0:\n",
    "            train_g=dgl.add_nodes(train_g, len(target_new_nodes), ntype='target')\n",
    "            train_g.nodes['target'].data['features'][-len(new_features):,:]=new_features\n",
    "            added_nodes['target']=(list(target_new_nodes), target_new_vals)\n",
    "\n",
    "        if len(target_nodes_to_add)>0:\n",
    "            train_g = dgl.add_edges(train_g, target_nodes_to_add, target_nodes_to_add, etype=('target', 'self_relation', 'target'))\n",
    "\n",
    "        for nc in node_cols.split(','):\n",
    "            nodes, lookup, new_nodes, new_vals = nodes_to_ids(test_df[nc],\n",
    "                                                                 nodes_lookup[nc],\n",
    "                                                                 train_g.number_of_nodes(nc))\n",
    "\n",
    "            if len(new_nodes)> 0:\n",
    "                train_g = dgl.add_nodes(train_g, len(new_nodes), ntype=nc)\n",
    "                added_nodes[nc] = (new_nodes, new_vals)\n",
    "\n",
    "            elist_u = []\n",
    "            elist_v = []\n",
    "            rlist_u = []\n",
    "            rlist_v = []\n",
    "            for s, t in zip(nodes, target_nodes):\n",
    "                if t in target_new_nodes:\n",
    "                    elist_u.append(t)\n",
    "                    elist_v.append(s)\n",
    "                    rlist_u.append(s)\n",
    "                    rlist_v.append(t)\n",
    "\n",
    "            if len(elist_u)>0:\n",
    "                train_g = dgl.add_edges(train_g, elist_u, elist_v, etype=('target', f'target<>{nc}', nc))\n",
    "                train_g = dgl.add_edges(train_g, rlist_u, rlist_v, etype=(nc, f'{nc}<>target', 'target'))\n",
    "\n",
    "        return train_g, target_nodes, added_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff5a6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train_g, trained_model, test_df: pd.DataFrame, nodes_lookup, feature_processor, train_mean, train_stdev, k: int = 2):\n",
    "        #Function to perform model inference on extracted test subgraph and get model predictions for test nodes\n",
    "\n",
    "        if train_g is None:\n",
    "            raise RuntimeError(\"Model must be trained first!\")\n",
    "\n",
    "        device = th.device('cpu')\n",
    "        \n",
    "        #Extend the training graph using the new nodes and get the test subgraph for inference\n",
    "        train_g, target_nodes, added_nodes = extend_graph(test_df, train_g, parameters['target_col'], \n",
    "                                                          parameters['node_cols'], nodes_lookup, \n",
    "                                                          cat_transformer, train_mean, train_stdev)\n",
    "\n",
    "        test_g, inverse_target_nodes = dgl.khop_out_subgraph(train_g, {'target': target_nodes}, k=k)\n",
    "\n",
    "        test_features = test_g.nodes['target'].data['features']\n",
    "        test_features = test_features.to(device)\n",
    "\n",
    "        train_n_nodes = th.sum(th.tensor([train_g.number_of_nodes(n_type) for n_type in train_g.ntypes]))\n",
    "\n",
    "        test_n_nodes = th.sum(th.tensor([test_g.number_of_nodes(n_type) for n_type in test_g.ntypes]))\n",
    "        test_n_edges = th.sum(th.tensor([test_g.number_of_edges(e_type) for e_type in test_g.etypes]))\n",
    "\n",
    "        print(\"\"\"----Test subgraph extracted------'\n",
    "                    No. of Nodes: {}\n",
    "                    No. of Edges: {}\"\"\".format(test_n_nodes,test_n_edges,))\n",
    "\n",
    "        model = trained_model\n",
    "        embed_copy = dict(model.embed)\n",
    "\n",
    "        print(\"Started model inference\")\n",
    "        for ntype, emb_ in model.embed.items():\n",
    "            train_num = train_g.number_of_nodes(ntype)\n",
    "            test_num = test_g.number_of_nodes(ntype)\n",
    "\n",
    "            mean_emb = th.mean(emb_, dim=0)\n",
    "\n",
    "            new_emb = mean_emb.repeat(test_num, 1).detach().numpy()\n",
    "\n",
    "            ### for nodes in subgraph, get their node-ids in train_g (full graph)\n",
    "            train_g_ids = test_g.ndata[dgl.NID][ntype].numpy()\n",
    "\n",
    "            ### filter out subgraph nodes that were added to train_g after training,\n",
    "            ### Remaining nodes are the ones that will have learned embeddings\n",
    "            emb_train_g_ids = np.where(train_g_ids<emb_.shape[0])[0]\n",
    "\n",
    "            print(f\"Number of nodes of type {ntype} in subgraph={test_num}\")\n",
    "\n",
    "            ### copy embeddings of the subgraph nodes that were \"seen\" during training\n",
    "            ### and fill the embeddings of other nodes in subgraph with  the mean\n",
    "            new_emb[emb_train_g_ids, :] = th.index_select(emb_, 0, th.from_numpy(train_g_ids[emb_train_g_ids])).detach().numpy()\n",
    "\n",
    "            model.embed[ntype] = th.nn.Parameter(th.from_numpy(new_emb))\n",
    "\n",
    "\n",
    "        raw_preds = model(test_g, test_features.to(device))\n",
    "        pred_prob = th.softmax(raw_preds, dim=-1)\n",
    "        fraud_prob = pred_prob[:, 1].detach().numpy()\n",
    "\n",
    "        model.embed = th.nn.ParameterDict(embed_copy)\n",
    "\n",
    "        ### clean-up graph: remove newly added nodes from graph and from node-id lookups\n",
    "        for ntype, nodes_tup in added_nodes.items():\n",
    "            new_node_ids, new_node_vals = nodes_tup\n",
    "            train_g.remove_nodes(new_node_ids, ntype=ntype)\n",
    "\n",
    "            for new_val in new_node_vals:\n",
    "                del nodes_lookup[ntype][new_val]\n",
    "\n",
    "        return fraud_prob[inverse_target_nodes['target'].numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "998089f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amsha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:329: RuntimeWarning: invalid value encountered in log10\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Test subgraph extracted------'\n",
      "                    No. of Nodes: 276808\n",
      "                    No. of Edges: 3317359\n",
      "Started model inference\n",
      "Number of nodes of type P_emaildomain in subgraph=60\n",
      "Number of nodes of type ProductCD in subgraph=4\n",
      "Number of nodes of type R_emaildomain in subgraph=61\n",
      "Number of nodes of type addr1 in subgraph=60702\n",
      "Number of nodes of type addr2 in subgraph=60519\n",
      "Number of nodes of type card1 in subgraph=8499\n",
      "Number of nodes of type card2 in subgraph=1384\n",
      "Number of nodes of type card3 in subgraph=275\n",
      "Number of nodes of type card4 in subgraph=5\n",
      "Number of nodes of type card5 in subgraph=1062\n",
      "Number of nodes of type card6 in subgraph=4\n"
     ]
    }
   ],
   "source": [
    "#Model evaluation on test set\n",
    "n_hops=3\n",
    "fraud_prob=predict(train_g, trained_model, test_df, nodes_lookup, feature_processor, train_mean, train_stdev, k=n_hops)\n",
    "auc=roc_auc_score(test_df.isFraud, fraud_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0495ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score = 0.8696365077867189\n",
      "Accuracy Score = 0.931604672929594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "print(f\"AUC Score = {roc_auc_score(test_df.isFraud, fraud_prob)}\")\n",
    "print(f\"Accuracy Score = {accuracy_score(test_df.isFraud, np.round(fraud_prob))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4b3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
